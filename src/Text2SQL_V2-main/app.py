from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import os
import logging

def setup_logger():
    """
    Sets up a logger with a console handler.
    """
    # Create a logger
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)  # Set minimum log level

    # Prevent adding multiple handlers if setup_logger is called multiple times
    if not logger.handlers:
        # Create console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.DEBUG)  # Handler log level

        # Create formatter for log messages
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)

        # Add handler to logger
        logger.addHandler(console_handler)

    return logger
logger = setup_logger()

# Load environment variables from .env file FIRST (before any other imports that might need them)
from dotenv import load_dotenv
load_dotenv()

# Import config to ensure .env is loaded and validate API keys
from config import config

# Original imports 
from core.db_builder import load_schema, build_database, execute_sql
from core.schema_loader import SchemaLoader

# New global data layer
from core.data_layer import global_data_layer
from core.api_service import FactoryKPIService, DCKPIService, StoreKPIService, NodeHealthService, GlobalCommandCenterService

# Agents
from agents.text2sql_agent import Text2SQLAgent
from agents.summarizer_agent import SummarizerAgent

# Utility for chart intent detection
from utils.intent import wants_chart

from utils.persist import persist_order_log

# Email & Summary (V2-main features)
from mailer import send_success_email
from summary_generator import generate_llm_summary


app = Flask(__name__)
# Enable CORS for frontend requests - allow all origins
# This allows requests from localhost (dev) and any deployed frontend
CORS(app, 
     resources={r"/*": {
         "origins": "*",
         "methods": ["GET", "POST", "OPTIONS"],
         "allow_headers": ["Content-Type", "Authorization"],
         "expose_headers": ["Content-Type"]
     }}, 
     supports_credentials=True)

# Add CORS headers to all responses
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

# Get the directory where app.py is located
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ---------------------------------------------
# Initialize Global Data Layer
# ---------------------------------------------
# This loads all CSVs, validates data, and precomputes all KPIs
# This is the single source of truth for the entire application
try:
    global_data_layer.initialize(BASE_DIR)
    logger.info("✅ Global data layer initialized successfully")
except Exception as e:
    logger.info(f"⚠️  Warning: Failed to initialize data layer: {str(e)}")
    logger.info("⚠️  REST API endpoints may not work correctly. Text2SQL chatbot will still function.")

# ---------------------------------------------
# LOAD SCHEMA
# ---------------------------------------------
# Use absolute paths based on app.py location
schema = [
    {
        "table_name": "dc_168h_forecasts",
        "path": os.path.join(BASE_DIR, "datasets", "dc_168h_forecasts.csv"),
    },
    {
        "table_name": "store_168h_forecasts",
        "path": os.path.join(BASE_DIR, "datasets", "store_168h_forecasts.csv"),
    },
    {
        "table_name": "order_log",
        "path": os.path.join(BASE_DIR, "datasets", "order_log.csv"),
    },
]

# Use your existing schema loader
schema_loader = SchemaLoader(schema)
loaded_schema = schema_loader.load()

# Build local SQLite DB - use absolute path
db_path = os.path.join(BASE_DIR, "local.db")
try:
    build_database(schema, db_path)
    logger.info("✅ Database built successfully")
except Exception as e:
    logger.info(f"⚠️  Warning: Database build failed: {str(e)}")

# Load schema metadata for Text2SQLAgent
schema_metadata_path = os.path.join(BASE_DIR, "schema_metadata.json")
schema_metadata = {}
try:
    if os.path.exists(schema_metadata_path):
        with open(schema_metadata_path, "r") as f:
            schema_metadata = json.load(f)
        logger.info("✅ Schema metadata loaded successfully")
    else:
        logger.info(f"⚠️  Warning: schema_metadata.json not found at {schema_metadata_path}")
except Exception as e:
    logger.info(f"⚠️  Warning: Failed to load schema metadata: {str(e)}")


# ---------------------------------------------
# Initialize agents (with error handling)
# ---------------------------------------------
t2s = None
summarizer = None
agent_error = None

try:
    # Verify API key is loaded before initializing agents
    if config.LLM_PROVIDER == "google":
        if not config.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY is required but not set. Please check your .env file.")
        logger.info(f"✅ Using Google Gemini API (model: {config.GOOGLE_MODEL})")
        if config.GOOGLE_API_KEY and len(config.GOOGLE_API_KEY) > 8:
            masked_key = '*' * (len(config.GOOGLE_API_KEY) - 8) + config.GOOGLE_API_KEY[-8:]
        else:
            masked_key = 'NOT SET' if not config.GOOGLE_API_KEY else 'SET (too short to mask)'
        logger.info(f"✅ API Key loaded: {masked_key}")
    elif config.LLM_PROVIDER == "openai":
        if not config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required but not set. Please check your .env file.")
        logger.info(f"✅ Using OpenAI API (model: {config.OPENAI_MODEL})")
        if config.OPENAI_API_KEY and len(config.OPENAI_API_KEY) > 8:
            masked_key = '*' * (len(config.OPENAI_API_KEY) - 8) + config.OPENAI_API_KEY[-8:]
        else:
            masked_key = 'NOT SET' if not config.OPENAI_API_KEY else 'SET (too short to mask)'
        logger.info(f"✅ API Key loaded: {masked_key}")
    
    t2s = Text2SQLAgent(db_path, loaded_schema, schema_metadata)
    summarizer = SummarizerAgent()
    logger.info("✅ Agents initialized successfully")
except Exception as e:
    agent_error = str(e)
    logger.info(f"⚠️  Warning: Failed to initialize agents: {agent_error}")
    logger.info("⚠️  The /query endpoint will return errors until agents are properly configured.")
    logger.info("⚠️  Please check your .env file for LLM_PROVIDER and API keys.")
    logger.info(f"⚠️  Current LLM_PROVIDER: {config.LLM_PROVIDER}")
    logger.info(f"⚠️  GOOGLE_API_KEY set: {bool(config.GOOGLE_API_KEY)}")
    logger.info(f"⚠️  OPENAI_API_KEY set: {bool(config.OPENAI_API_KEY)}")

# ---------------------------------------------
# Text2SQL query endpoint (with V2 email functionality)
# ---------------------------------------------
@app.route("/query", methods=["POST", "OPTIONS"])
def query():
    # Handle OPTIONS request for CORS preflight
    if request.method == "OPTIONS":
        return jsonify({}), 200
    
    try:
        # Check if agents are initialized
        if t2s is None or summarizer is None:
            error_msg = agent_error or "Agents not initialized. Please check backend configuration."
            return jsonify({
                "error": "Agents not available",
                "details": error_msg,
                "sql": None,
                "data": [],
                "summary": "The AI agents are not properly configured. Please check the backend logs for details. Common issues: missing API keys (GOOGLE_API_KEY or OPENAI_API_KEY) in environment variables.",
                "viz": None,
                "mime": None
            }), 503
        
        # Validate request body
        if not request.json:
            return jsonify({"error": "Request body is required"}), 400

        body = request.json
        question = body.get("question", "").strip()
        
        if not question:
            return jsonify({"error": "Question is required"}), 400

        # Step 1 — Get SQL from text2sql agent
        try:
            sql = t2s.run(question)
        except Exception as e:
            logger.info(f"Error in Text2SQL agent: {str(e)}")
            return jsonify({
                "error": "Failed to generate SQL query",
                "details": str(e),
                "sql": None,
                "data": [],
                "summary": "I encountered an error while processing your query. Please try rephrasing it.",
                "viz": None,
                "mime": None
            }), 500

        # Step 2 — Execute SQL
        try:
            result = execute_sql(db_path, sql)
            # Log query result info for debugging
            if hasattr(result, 'empty'):
                logger.info(f"Query returned {len(result)} rows. Empty: {result.empty}")
                if not result.empty:
                    logger.info(f"Columns: {result.columns.tolist()}")
                    logger.info(f"Sample data:\n{result.head(3).to_string()}")
            else:
                logger.info(f"Query result type: {type(result)}, value: {result}")
        except Exception as e:
            logger.info(f"Error executing SQL: {str(e)}")
            logger.info(f"Generated SQL: {sql}")
            return jsonify({
                "error": "Failed to execute SQL query",
                "details": str(e),
                "sql": sql,
                "data": [],
                "summary": "I generated a SQL query but encountered an error executing it. Please try rephrasing your question.",
                "viz": None,
                "mime": None
            }), 500

        # WRITE query (INSERT / UPDATE / DELETE) - V2-main email functionality
        if isinstance(result, int):
            rows_affected = result
            
            # Generate LLM email summary (V2-main feature)
            try:

                email_content = generate_llm_summary(sql, rows_affected)
                logger.info("✅ Email summary generated successfully")
            except Exception as e:
                logger.info(f"Error generating email summary: {str(e)}")
                

            # Persist transactional table and send email
            try:
                persist_order_log(db_path)
                logger.info("✅ Order log persisted successfully")
            except Exception as e:
                logger.info(f"Warning: Failed to persist order_log: {str(e)}")
            
            # Send email notification (V2-main feature)
            try:
                send_success_email(
                    subject=email_content["subject"],
                    body=email_content["body"]
                )
                logger.info("✅ Success email sent successfully")
            except Exception as e:
                logger.info(f"Warning: Failed to send email: {str(e)}")

            # Return response with email fields (V2-main format)
            return jsonify({
                "sql": sql,
                "rows_affected": rows_affected,
                "email_subject": email_content["subject"],
                "email_body": email_content["body"],
                "summary": f"{rows_affected} row(s) successfully written to order_log.",
                "data": [],
                "viz": None,
                "mime": None
            })

        # READ query (SELECT)
        else:
            df = result
            # Step 3 — Summarize result
            # Only summarize if we have data
            if df.empty:
                summary = f"No data found for your query: '{question}'. Please try rephrasing your question or check if the data exists in the database."
                data = []
            else:
                try:
                    summary = summarizer.summarize(question, df)
                except Exception as e:
                    logger.info(f"Error summarizing results: {str(e)}")
                    # Use a fallback summary if summarization fails
                    summary = f"Query returned {len(df)} row(s). Columns: {', '.join(df.columns.tolist()[:5])}"

                data = df.to_dict(orient="records")

            # Step 4 — Generate visualization ONLY if explicitly asked
            viz, mime = None, None
            if wants_chart(question) and not df.empty:
                try:
                    logger.info(f"Generating visualization for query: '{question}'")
                    logger.info(f"Data shape: {df.shape}, Columns: {df.columns.tolist()}")
                    viz, mime = summarizer.generate_viz(question, df)
                    if viz:
                        logger.info(f"✅ Visualization generated successfully (size: {len(viz)} chars, mime: {mime})")
                    else:
                        logger.info(f"⚠️  Visualization generation returned None (no error thrown)")
                except Exception as e:
                    logger.info(f"❌ Error generating visualization: {str(e)}")
                    import traceback
                    traceback.logger.info_exc()
                    # Continue without visualization if it fails

            return jsonify({
                "sql": sql,
                "data": data,
                "summary": summary,
                "viz": viz,
                "mime": mime
            })
    
    except Exception as e:
        logger.info(f"Unexpected error in /query endpoint: {str(e)}")
        import traceback
        traceback.logger.info_exc()
        return jsonify({
            "error": "Internal server error",
            "details": str(e),
            "sql": None,
            "data": [],
            "summary": "I encountered an unexpected error. Please try again.",
            "viz": None,
            "mime": None
        }), 500


# ---------------------------------------------
# Store KPIs endpoint (Store Operations)
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/store-kpis", methods=["GET"])
def store_kpis():
    """
    Get store-level KPIs from the precomputed intermediate dataframe.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    store_id = request.args.get("store_id", "ST_DUBAI_HYPER_01")
    result = StoreKPIService.get_store_kpis(store_id=store_id)
    return jsonify(result)


@app.route("/store-shelf-performance", methods=["GET"])
def store_shelf_performance():
    """
    Get shelf performance data (SKU-level) for a specific store.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    store_id = request.args.get("store_id", "ST_DUBAI_HYPER_01")
    results = StoreKPIService.get_store_shelf_performance(store_id=store_id)
    return jsonify(results)


@app.route("/store-hourly-sales", methods=["GET"])
def store_hourly_sales():
    """
    Get hourly sales vs forecast data from historical_vs_model_comparison.csv.
    
    Aggregates actual_sales and predicted_demand by hour for a specific store.
    """
    import pandas as pd
    
    store_id = request.args.get("store_id", "ST_DUBAI_HYPER_01")
    
    try:
        # Load the CSV file
        csv_path = os.path.join(BASE_DIR, "datasets", "historical_vs_model_comparison.csv")
        
        if not os.path.exists(csv_path):
            logger.warning(f"CSV file not found: {csv_path}")
            return jsonify({"error": "Historical comparison data not available"}), 404
        
        # Read CSV
        df = pd.read_csv(csv_path)
        
        # Filter by store_id
        df_filtered = df[df["store_id"] == store_id].copy()
        
        if df_filtered.empty:
            logger.warning(f"No data found for store_id: {store_id}")
            return jsonify([])
        
        # Convert timestamp to datetime
        df_filtered["timestamp"] = pd.to_datetime(df_filtered["timestamp"])
        
        # Extract hour from timestamp
        df_filtered["hour"] = df_filtered["timestamp"].dt.hour
        
        # Aggregate by hour: sum actual_sales and predicted_demand
        hourly_data = df_filtered.groupby("hour").agg({
            "actual_sales": "sum",
            "predicted_demand": "sum"
        }).reset_index()
        
        # Format hour as "HH:00"
        hourly_data["hour"] = hourly_data["hour"].apply(lambda x: f"{x:02d}:00")
        
        # Rename columns to match frontend expectations
        hourly_data = hourly_data.rename(columns={
            "actual_sales": "sales",
            "predicted_demand": "forecast"
        })
        
        # Convert to list of dictionaries
        result = hourly_data[["hour", "sales", "forecast"]].to_dict(orient="records")
        
        # Ensure all 24 hours are present (fill missing hours with 0)
        all_hours = [f"{h:02d}:00" for h in range(24)]
        existing_hours = {item["hour"]: item for item in result}
        complete_result = [
            existing_hours.get(hour, {"hour": hour, "sales": 0, "forecast": 0})
            for hour in all_hours
        ]
        
        return jsonify(complete_result)
        
    except Exception as e:
        logger.error(f"Error loading hourly sales data: {str(e)}")
        return jsonify({"error": f"Failed to load hourly sales data: {str(e)}"}), 500


# ---------------------------------------------
# DC Inventory Age Distribution endpoint
# ---------------------------------------------
@app.route("/dc-inventory-age", methods=["GET"])
def dc_inventory_age():
    """
    Get inventory age distribution for a specific DC.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    dc_id = request.args.get("dc_id")
    results = DCKPIService.get_dc_inventory_age_distribution(dc_id=dc_id)
    return jsonify(results)


# ---------------------------------------------
# DC KPIs endpoint (Distribution Center)
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/dc-kpis", methods=["GET"])
def dc_kpis():
    """
    Get DC-level KPIs from the precomputed intermediate dataframe.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    dc_id = request.args.get("dc_id", "DC_JEDDAH")
    result = DCKPIService.get_dc_kpis(dc_id=dc_id)
    return jsonify(result)


# ---------------------------------------------
# DC Days-of-Cover endpoint (per DC, per SKU)
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/dc-days-cover", methods=["GET"])
def dc_days_cover():
    """
    Get days-of-cover per (dc_id, sku_id) from the precomputed intermediate dataframe.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    dc_id = request.args.get("dc_id")
    sku_id = request.args.get("sku_id")
    results = DCKPIService.get_dc_days_cover(dc_id=dc_id, sku_id=sku_id)
    return jsonify(results)


# ---------------------------------------------
# Factory KPIs endpoint (Factory Control Tower)
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/factory-kpis", methods=["GET"])
def factory_kpis():
    """
    Get factory-level KPIs from the precomputed intermediate dataframe.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    factory_id = request.args.get("factory_id")
    line_id = request.args.get("line_id")
    result = FactoryKPIService.get_factory_kpis(factory_id=factory_id, line_id=line_id)
    return jsonify(result)


@app.route("/factory-hourly-production", methods=["GET"])
def factory_hourly_production():
    """
    Get hourly production data (actual and demand) for a factory/line.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    factory_id = request.args.get("factory_id")
    line_id = request.args.get("line_id")
    results = FactoryKPIService.get_factory_hourly_production(factory_id=factory_id, line_id=line_id)
    return jsonify(results)

@app.route("/factory-dispatch-planning", methods=["GET"])
def factory_dispatch_planning():
    """
    Get dispatch planning data (SKU-level production recommendations) for a factory/line.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    factory_id = request.args.get("factory_id")
    line_id = request.args.get("line_id")
    results = FactoryKPIService.get_factory_dispatch_planning(factory_id=factory_id, line_id=line_id)
    return jsonify(results)


# ---------------------------------------------
# Node Health Summary endpoint
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/node-health", methods=["GET"])
def node_health():
    """
    Get node health summary for all nodes (Factory, DC, Store).
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    results = NodeHealthService.get_node_health()
    return jsonify(results)


# ---------------------------------------------
# Global Command Center KPIs endpoint
# Now uses the global intermediate dataframe layer
# ---------------------------------------------
@app.route("/global-kpis", methods=["GET"])
def global_kpis():
    """
    Get global Command Center KPIs aggregated across Factory, DC, and Store.
    
    All business logic is in the data layer - this endpoint is purely presentational.
    """
    results = GlobalCommandCenterService.get_global_kpis()
    return jsonify(results)


# ---------------------------------------------
# Health check endpoint
# ---------------------------------------------
@app.route("/health", methods=["GET", "OPTIONS"])
def health():
    """Health check endpoint to verify backend is running"""
    agents_status = "ready" if (t2s is not None and summarizer is not None) else "not_initialized"
    return jsonify({
        "status": "healthy",
        "service": "al-hatab-insights-backend",
        "version": "2.0.0",
        "agents": agents_status,
        "agent_error": agent_error if agent_error else None
    }), 200


# Legacy health check endpoint (for backwards compatibility)
@app.route("/", methods=["GET"])
def health_check():
    return jsonify({"status": "ok", "message": "Text2SQL API is running"})


if __name__ == "__main__":
    # Use PORT environment variable for Render deployment, fallback to 5000 for local development
    port = int(os.environ.get("PORT", 5000))
    app.run(debug=False, host="0.0.0.0", port=port)
